{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c03919",
   "metadata": {},
   "source": [
    "# Getting started with langchain - question answering\n",
    "\n",
    "This notebook walks through how to use LangChain for question answering over a given document. \n",
    "\n",
    "Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. However, using these LLMs in isolation is often insufficient for creating a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\n",
    "\n",
    "And that is what we will do here!\n",
    "\n",
    "### Sources\n",
    "- GitHub: https://github.com/hwchase17/langchain\n",
    "- Documents: https://python.langchain.com/en/latest/use_cases/question_answering.html\n",
    "- PyPi: https://pypi.org/project/langchain/\n",
    "\n",
    "### Contents\n",
    "0. Install packages\n",
    "1. Prepare data\n",
    "2. Search the document\n",
    "2. Call the LLM to do generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51977840",
   "metadata": {},
   "source": [
    "## 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f4996",
   "metadata": {},
   "source": [
    "## 1. Prepare Data\n",
    "First we prepare the data. For this example we do similarity search over a vector database, but these documents could be fetched in any manner (the point of this notebook to highlight what to do AFTER you fetch the documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17fcbc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350304b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store your openai api key as an environment variable - as required by OpenAI (see previous tutorial)\n",
    "import os\n",
    "import config # I've created a config.py file that stores my password\n",
    "os.environ['OPENAI_API_KEY'] = config.openai_key #but don't keep it in your source code :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cac679f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown\r",
      "-1 / unknown"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'state_of_the_union (3).txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will use the wget library to download the \n",
    "import wget\n",
    "filename = wget.download('https://github.com/hwchase17/chat-your-data/blob/master/state_of_the_union.txt')\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef9305cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "\n",
    "#create the embeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23839a9c",
   "metadata": {},
   "source": [
    "## 2. Search the document\n",
    "A first step can be to search your document. This way we will find the location in the document where words of the query are mentioned. The result of this search is the relevant part of the document\n",
    "(In the next step we will use this result and feed it to the LLM so it can generate a nice response.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291f0117",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "#first we will create docsearch variable. \n",
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1eaf6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#first tr\n",
    "query = \"What did the president say about Justice Breyer\"\n",
    "docs = docsearch.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0cddfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': '31'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the best result\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64b7f8",
   "metadata": {},
   "source": [
    "## 3. Call the LLM to generate an answer\n",
    "If you just want to get started as quickly as possible, this is the fasted way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a16e3453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import two more liberies\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd9e6190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The president said that Justice Breyer is an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court, and thanked him for his service.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the chain, give the pro\n",
    "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "#create your prompt\n",
    "query = \"What did the president say about Justice Breyer\"\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea01309",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "That's it! We've taken a text document from the web and did document search with it. Also we used a Large Language Model to generate a good answer for it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
