{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SslnlBaKPeWP"
   },
   "source": [
    "# Wikipedia Semantic Search with Cohere Embedding Archives\n",
    "This notebook contains the starter code to do simple [semantic search](https://txt.cohere.ai/what-is-semantic-search/) on the [Wikipedia embeddings archives](https://txt.cohere.ai/embeddings-archives-wikipedia/) published by Cohere. These archives embed Wikipedia sites in multiple languages. In this example, we'll use [Wikipedia Simple English](https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://github.com/cohere-ai/notebooks/blob/main/notebooks/Wikipedia_Semantic_Search_With_Cohere_Embeddings_Archives.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IUnwp2cYNnP0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cohere\n",
      "  Downloading cohere-4.3.1-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: datasets in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from cohere) (2.28.1)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from cohere) (2.2.1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from cohere) (3.8.4)\n",
      "Requirement already satisfied: pandas in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: responses<0.19 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: packaging in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: multiprocess in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
      "Requirement already satisfied: filelock in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests<3.0,>=2.0->cohere) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests<3.0,>=2.0->cohere) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from requests<3.0,>=2.0->cohere) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: cohere\n",
      "Successfully installed cohere-4.3.1\n"
     ]
    }
   ],
   "source": [
    "# Let's install cohere and HuggingFace datasets\n",
    "!pip install cohere datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZds1apHPsag"
   },
   "source": [
    "Let's now download 1,000 records from the English Wikipedia embeddings archive so we can search it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "v8Pogz7gPQwg"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import cohere\n",
    "\n",
    "# Add your cohere API key from www.cohere.com\n",
    "co = cohere.Client(\"kWWNHxy1gyMljLopnNMMXxLP0sN3fO7u94lJV2xd\")  \n",
    "\n",
    "#Load at max 10000 documents + embeddings\n",
    "max_docs = 10000\n",
    "docs_stream = load_dataset(f\"Cohere/wikipedia-22-12-en-embeddings\", split=\"train\", streaming=True)\n",
    "\n",
    "docs = []\n",
    "doc_embeddings = []\n",
    "\n",
    "for doc in docs_stream:\n",
    "    docs.append(doc)\n",
    "    doc_embeddings.append(doc['emb'])\n",
    "    if len(docs) >= max_docs:\n",
    "        break\n",
    "\n",
    "doc_embeddings = torch.tensor(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIlx5RVCP7g7"
   },
   "source": [
    "Now, `doc_embeddings` holds the embeddings of the first 1,000 documents in the dataset. Each document is represented as an [embeddings vector](https://txt.cohere.ai/sentence-word-embeddings/) of 768 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBa3oxSsP2fv",
    "outputId": "d9d71135-7ac3-4424-d806-2a994a0b456a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 768])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[ 0.2866, -0.0318,  0.0667,  ...,  0.0602, -0.2362, -0.0712],\n",
      "        [-0.0969,  0.1619, -0.0980,  ...,  0.2985,  0.0398,  0.0344],\n",
      "        [ 0.1302,  0.2657,  0.4018,  ...,  0.2803, -0.0346,  0.1877],\n",
      "        ...,\n",
      "        [ 0.1199, -0.5237,  0.1080,  ...,  0.1375, -0.1667, -0.3578],\n",
      "        [-0.1171,  0.1487, -0.5787,  ..., -0.3205, -0.4541,  0.0279],\n",
      "        [ 0.0129, -0.1160,  0.0472,  ..., -0.1739, -0.1747, -0.3018]])\n"
     ]
    }
   ],
   "source": [
    "#this is the first batch of 1,000 articles\n",
    "print(doc_embeddings.shape)\n",
    "print(type(doc_embeddings))\n",
    "print(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the object is 72 bytes\n"
     ]
    }
   ],
   "source": [
    "#these embeddings are really small! Get the size of 1,000 embeddings isn bytes\n",
    "import sys\n",
    "print(f'The size of the object is {sys.getsizeof(doc_embeddings)} bytes, but this is not the size of the embeddings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO GET SIZE OF EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbYAXaI4RQiH"
   },
   "source": [
    "We can now search these vectors for any query we want. For this toy example, we'll ask a question about Wikipedia since we know the Wikipedia page is included in the first 1000 documents we used here.\n",
    "\n",
    "To search, we embed the query, then get the nearest neighbors to its embedding (using dot product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJGUurziNiYR",
    "outputId": "bb66def9-3d83-46f7-c871-1224eb5714cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who founded Wikipedia\n",
      "Wikipedia\n",
      "Various collaborative online encyclopedias were attempted before the start of Wikipedia, but with limited success. Wikipedia began as a complementary project for Nupedia, a free online English-language encyclopedia project whose articles were written by experts and reviewed under a formal process. It was founded on March 9, 2000, under the ownership of Bomis, a web portal company. Its main figures were Bomis CEO Jimmy Wales and Larry Sanger, editor-in-chief for Nupedia and later Wikipedia. Nupedia was initially licensed under its own Nupedia Open Content License, but before Wikipedia was founded, Nupedia switched to the GNU Free Documentation License at the urging of Richard Stallman. Wales is credited with defining the goal of making a publicly editable encyclopedia, while Sanger is credited with the strategy of using a wiki to reach that goal. On January 10, 2001, Sanger proposed on the Nupedia mailing list to create a wiki as a \"feeder\" project for Nupedia. \n",
      "\n",
      "Wikipedia\n",
      "Wikipedia was launched by Jimmy Wales and Larry Sanger on January 15, 2001. Sanger coined its name as a blend of \"wiki\" and \"encyclopedia\". Wales was influenced by the \"spontaneous order\" ideas associated with Friedrich Hayek and the Austrian School of economics after being exposed to these ideas by the libertarian economist Mark Thornton. Initially available only in English, versions in other languages were quickly developed. Its combined editions comprise more than articles, attracting around 2billion unique device visits per month and more than 17 million edits per month (1.9edits per second) . In 2006, \"Time\" magazine stated that the policy of allowing anyone to edit had made Wikipedia the \"biggest (and perhaps best) encyclopedia in the world\". \n",
      "\n",
      "Wikipedia\n",
      "The domains \"wikipedia.com\" (later redirecting to \"wikipedia.org\") and \"wikipedia.org\" were registered on January 12, 2001, and January 13, 2001, respectively, and Wikipedia was launched on January 15, 2001, as a single English-language edition at www.wikipedia.com, and announced by Sanger on the Nupedia mailing list. Its integral policy of \"neutral point-of-view\" was codified in its first few months. Otherwise, there were initially relatively few rules, and it operated independently of Nupedia. Bomis originally intended it as a business for profit. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the query, then embed it\n",
    "query = 'Who founded Wikipedia'\n",
    "response = co.embed(texts=[query], model='multilingual-22-12')\n",
    "query_embedding = response.embeddings \n",
    "query_embedding = torch.tensor(query_embedding)\n",
    "\n",
    "# Compute dot score between query embedding and document embeddings\n",
    "dot_scores = torch.mm(query_embedding, doc_embeddings.transpose(0, 1))\n",
    "top_k = torch.topk(dot_scores, k=3)\n",
    "\n",
    "# Print results\n",
    "print(\"Query:\", query)\n",
    "for doc_id in top_k.indices[0].tolist():\n",
    "    print(docs[doc_id]['title'])\n",
    "    print(docs[doc_id]['text'], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWFroOO2RwMd"
   },
   "source": [
    "This shows the top three passages that are relevant to the query. We can retrieve more results by changing the `k` value. The question in this simple demo is about Wikipedia because we know that the Wikipedia page is part of the documents in this subset of the archive."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
